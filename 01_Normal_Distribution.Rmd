---
title: "Chapter 1 - Let's Explore the Normal Distribution"
author: "Colin Quirk"
date: "`r format(Sys.Date())`"
output:
  github_document
---

# Let's Explore the Normal Distribution

```{r setup, include=FALSE}
library(knitr)
library(r2d3)
library(htmlwidgets)

opts_chunk$set(echo = FALSE)
```


The normal or Gaussian distribution is extremely important as it shows up all the time in nature. It is controlled by two parameters, a mean μ and a standard deviation σ. The standard normal is defined as a normal distribution with μ=0 and σ=1. The importance of this special case will be discussed towards the end of this chapter, but intuitively you can think of every other normal distribution as a mutation of the standard normal (where the mutation is defined by μ and σ).

You probably have explored the normal distribution before. Still, this is a great opportunity for you to play around with a widget in the context of this book. Below, you can adjust the parameters of the normal distribution and compare it to the standard normal.

<div id="standardNormalForm">
  <form>
    <input id="meanSlider" class="slider" type="range">
    <label id="meanSliderLabel" for="meanSlider">Mean: 2</label>
  </form>
  <form>
    <input id="sdSlider" class="slider" type="range">
    <label id="sdSliderLabel" for="sdSlider">Sd: 3.5</label>
  </form>
</div>

<center>
```{r normal_parameters}
r2d3(script="widgets/01-normal-parameters.js", width = 500, height = 400)
```
</center>

There's a few things that hopefully stick out after playing around with the parameters of the normal distribution. The first is that the mean shifts the distribution along the x axis. In contrast, the standard deviation affects the shape such that the larger the σ, the wider the shape. An important idea here is that the parameters are totally independent from each other. Knowing the position tells you nothing about the shape and vice versa.

## Why is the Normal Distribution so Common?

The normal distribution is extremely common in nature. In order to demonstrate why this is the case, we will perform a simulation. (The idea for this example came from McElreath's Statistical Rethinking, which I highly recommend you both read and watch in lecture form on youtube). In our example, we will pretend that you are holding a box of 1000 frogs in a long hallway. Unfortunately, you dropped the box and the frogs all fell on the ground in a straight line. Each second, each frog will randomly decide to stay still, hop to the left, or hop to the right. In the widget below, you can progress time with the "Progress" button or reset the simulation with the "Reset" button. Plotted are the physical location of each frog (left) and a histogram of the number of hops away from the starting point each frog is (right). Step through the simulation a few times and observe the results.

<center>
```{r frog_sim}
r2d3(script="widgets/01-frog-sim.js", width = 800, height = 500)
```

<form>
  <button id='progressFrogsButton'>Progress</button>
  <button id='resetFrogsButton'>Reset</button>
</form>

</center>

You might be surprised to see how quickly the normal distribution appears. Why is this? Let's consider the moment in time after each frog has made 8 random decisions. Possible values for the frogs' positions vary from -8 to 8, however it is very unlikely that a frog will reach the extremes. For example, to reach a value of 8, a frog would need to hop right (a $1/3$ chance) 8 total times. Basic probability tells us that the odds of this happening are $(1/3)^8$ i.e. a `r round((1/3)^8*100, digits=4)`% chance. Contrast this with the many different paths to end up at 0. A frog could choose to remain still for each of 8 decisions, but it could also hop right 4 times and then left 4 times or alternate between left and right, et cetera. Of the $3^8$ possible outcomes, only 1 leads to a value of 8, but N lead to a value of 0.

The normal distribution is observed whenever many small, independent variations are summed together to produce a value.


```{r}

```

